{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image scraper\n",
    "\n",
    "## Step 1: Create the Conda Environment\n",
    "Open your terminal (on macOS/Linux) or Anaconda Prompt (on Windows) and run the following command to create a new Conda environment named image_scraper with Python 3.11:\n",
    "\n",
    "```console\n",
    "conda create --name image_scraper python=3.11\n",
    "```\n",
    "\n",
    "## Step 2: Activate the Environment\n",
    "\n",
    "Activate the newly created environment using:\n",
    "\n",
    "```console\n",
    "conda activate image_scraper\n",
    "```\n",
    "\n",
    "## Step 3: Install Required Packages\n",
    "\n",
    "Install the necessary libraries for web scraping (Selenium and Requests), WebDriver Manager for Selenium driver management, and Jupyter to run and manage your notebooks. Here's the command sequence:\n",
    "\n",
    "```console\n",
    "conda install selenium requests jupyter -c conda-forge\n",
    "pip install webdriver-manager\n",
    "```\n",
    "\n",
    "Using -c conda-forge specifies that Conda should install these packages from the Conda-Forge repository, which often has more up-to-date packages than the default channel.\n",
    "\n",
    "## Step 4: Install Jupyter Kernel\n",
    "To make your image_scraper environment available as a kernel in Jupyter Notebooks, install ipykernel and add it as a kernel:\n",
    "\n",
    "```console\n",
    "conda install ipykernel -c conda-forge\n",
    "python -m ipykernel install --user --name=image_scraper --display-name=\"Python 3.11 (image_scraper)\"\n",
    "```\n",
    "\n",
    "This command adds the image_scraper environment as a kernel option in Jupyter, allowing you to select it when working on notebooks related to your image scraping project.\n",
    "\n",
    "## Step 5: Run Jupyter Notebook\n",
    "\n",
    "See code below for the image scraper:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting webdriver_manager\n",
      "  Downloading webdriver_manager-4.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\batkm\\anaconda3\\envs\\google-image-scraper\\lib\\site-packages (from webdriver_manager) (2.25.1)\n",
      "Collecting python-dotenv (from webdriver_manager)\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\batkm\\anaconda3\\envs\\google-image-scraper\\lib\\site-packages (from webdriver_manager) (23.2)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\batkm\\anaconda3\\envs\\google-image-scraper\\lib\\site-packages (from requests->webdriver_manager) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\batkm\\anaconda3\\envs\\google-image-scraper\\lib\\site-packages (from requests->webdriver_manager) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\batkm\\anaconda3\\envs\\google-image-scraper\\lib\\site-packages (from requests->webdriver_manager) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\batkm\\anaconda3\\envs\\google-image-scraper\\lib\\site-packages (from requests->webdriver_manager) (2024.2.2)\n",
      "Downloading webdriver_manager-4.0.1-py2.py3-none-any.whl (27 kB)\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv, webdriver_manager\n",
      "Successfully installed python-dotenv-1.0.1 webdriver_manager-4.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install webdriver_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded downloaded_images_flowers\\image_0.jpg\n",
      "Downloaded downloaded_images_flowers\\image_1.jpg\n",
      "Downloaded downloaded_images_flowers\\image_2.jpg\n",
      "Downloaded downloaded_images_flowers\\image_3.jpg\n",
      "Downloaded downloaded_images_flowers\\image_4.jpg\n",
      "Finished downloading images to downloaded_images_flowers.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "def download_image(image_url, directory, index):\n",
    "    \"\"\"\n",
    "    Downloads an image from a given URL into a specified directory.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(image_url)\n",
    "        if response.status_code == 200:\n",
    "            file_path = os.path.join(directory, f'image_{index}.jpg')\n",
    "            with open(file_path, 'wb') as file:\n",
    "                file.write(response.content)\n",
    "            print(f\"Downloaded {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {image_url}: {e}\")\n",
    "\n",
    "def scrape_images(search_term, limit):\n",
    "    \"\"\"\n",
    "    Scrapes images from DuckDuckGo based on a search term up to a specified limit.\n",
    "    \"\"\"\n",
    "    # Initialize the Chrome driver\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "\n",
    "    # Go to DuckDuckGo's image search page\n",
    "    driver.get(f\"https://duckduckgo.com/?q={search_term}&iax=images&ia=images\")\n",
    "    time.sleep(3)  # Allow time for the page to load\n",
    "\n",
    "    # Scroll the page to ensure that images are loaded\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(3)  # Wait for new images to load\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    # Find image elements on the page\n",
    "    images = driver.find_elements(By.CSS_SELECTOR, \"img.tile--img__img.js-lazyload\")\n",
    "    images = images[:limit]  # Limit the number of images to download\n",
    "\n",
    "    # Create a directory for the downloaded images\n",
    "    directory = f\"downloaded_images_{search_term.replace(' ', '_')}\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # Download the images\n",
    "    for index, image in enumerate(images):\n",
    "        image_url = image.get_attribute('src')\n",
    "        if image_url:\n",
    "            download_image(image_url, directory, index)\n",
    "\n",
    "    driver.quit()\n",
    "    print(f\"Finished downloading images to {directory}.\")\n",
    "\n",
    "# Example usage\n",
    "search_term = \"flowers\"  # Specify the search term here\n",
    "limit = 5  # Specify the maximum number of images to download\n",
    "\n",
    "scrape_images(search_term, limit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Deactivating the Environment\n",
    "\n",
    "Once you're done working in the image_scraper environment, you can deactivate it and return to your base environment by running:\n",
    "\n",
    "```console\n",
    "conda deactivate\n",
    "```\n",
    "\n",
    "## Important Considerations\n",
    "\n",
    "- This script is for educational purposes. Always respect the website's terms of service and copyright laws.\n",
    "- The performance and reliability of web scraping scripts can vary based on network conditions and changes to the target website's layout and design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image_scraper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
